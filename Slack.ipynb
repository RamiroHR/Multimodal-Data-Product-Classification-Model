{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e4970a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 54345 images belonging to 27 classes.\n",
      "Found 13587 images belonging to 27 classes.\n",
      "Found 16984 images belonging to 1 classes.\n",
      "Epoch 1/21\n",
      "  5/849 [..............................] - ETA: 1:08:12 - loss: 3.5304 - accuracy: 0.0656 "
     ]
    }
   ],
   "source": [
    "myseed = 123\n",
    "Nb_classes = 27\n",
    "\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(preprocessing_function = preprocess_input,\n",
    "                                   rescale = 1./255)\n",
    "\n",
    "val_datagen = ImageDataGenerator(preprocessing_function = preprocess_input,\n",
    "                                   rescale = 1./255)\n",
    "\n",
    "test_datagen = ImageDataGenerator(preprocessing_function = preprocess_input,\n",
    "                                   rescale = 1./255)\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "train_dataset = train_datagen.flow_from_directory( directory = '../Splitted_datasets/Images_224px/Train_subset',\n",
    "                                                   target_size = (224, 224),\n",
    "                                                   color_mode = 'rgb',\n",
    "                                                   batch_size = batch_size,\n",
    "                                                   class_mode = 'categorical',\n",
    "                                                   shuffle = False,\n",
    "                                                   seed = myseed)\n",
    "\n",
    "val_dataset = val_datagen.flow_from_directory( directory = '../Splitted_datasets/Images_224px/Validation_subset',\n",
    "                                                   target_size = (224, 224),\n",
    "                                                   color_mode = 'rgb',\n",
    "                                                   batch_size = batch_size,\n",
    "                                                   class_mode = 'categorical',\n",
    "                                                   shuffle = False,\n",
    "                                                   seed = myseed)\n",
    "\n",
    "test_dataset = test_datagen.flow_from_directory( directory = '../Splitted_datasets/Images_224px/Test_subset',\n",
    "                                                   target_size = (224, 224),\n",
    "                                                   color_mode = 'rgb',\n",
    "                                                   batch_size = 1,\n",
    "                                                   class_mode = None,\n",
    "                                                   shuffle = False,\n",
    "                                                   seed = myseed)\n",
    "\n",
    "### Found 54345 images belonging to 27 classes.\n",
    "### Found 13587 images belonging to 27 classes.\n",
    "### Found 16984 images belonging to 1 classes.\n",
    "\n",
    "\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "\n",
    "# get pre-rained VGG16 layers\n",
    "base_model = VGG16(weights = 'imagenet', include_top = False)\n",
    "\n",
    "# freeze layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# new sequential model\n",
    "VGG_model = Sequential()\n",
    "VGG_model.add(base_model)\n",
    "VGG_model.add(GlobalAveragePooling2D())\n",
    "VGG_model.add( Flatten() )\n",
    "VGG_model.add( Dense(units = 512, activation = 'relu') )\n",
    "VGG_model.add( Dropout(rate = 0.7) )\n",
    "VGG_model.add( Dense(units = 128, activation = 'relu') )\n",
    "VGG_model.add( Dense(units = Nb_classes, activation='softmax') )\n",
    "\n",
    "\n",
    "# checkpoint callbacks\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "checkpoint_filepath = './tmp_checkpoint/img_base_model_vgg'\n",
    "checkpoint = ModelCheckpoint(\n",
    "                            filepath=checkpoint_filepath,\n",
    "                            verbose = 1,\n",
    "                            save_weights_only=True,\n",
    "                            monitor='val_accuracy',\n",
    "                            mode='max',\n",
    "                            save_best_only=True)\n",
    "\n",
    "# reduce lr on plateau callback\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "lr_plateau = ReduceLROnPlateau(monitor = 'val_loss',\n",
    "                               patience=3,\n",
    "                               factor=0.5,\n",
    "                               verbose=2,\n",
    "                               mode='min',\n",
    "                               min_lr = 1e-10)\n",
    "\n",
    "Nb_epochs = 21\n",
    "lr_0 = 1e-3\n",
    "\n",
    "# compile model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "optimizer = Adam(learning_rate = lr_0)\n",
    "\n",
    "VGG_model.compile(loss = 'categorical_crossentropy',  # targets fromat is class numbers [2,3,22,..] from folder names.\n",
    "                  optimizer = optimizer,\n",
    "                  metrics = ['accuracy'])\n",
    "\n",
    "# train model\n",
    "STEP_SIZE_TRAIN=train_dataset.n//train_dataset.batch_size    ### 849\n",
    "STEP_SIZE_VALID=val_dataset.n//val_dataset.batch_size        ### 212\n",
    "\n",
    "training_history = VGG_model.fit(train_dataset,\n",
    "                                 steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                                 validation_data = val_dataset,\n",
    "                                 validation_steps=STEP_SIZE_VALID,\n",
    "                                 epochs = Nb_epochs,\n",
    "                                 callbacks = [\n",
    "                                             checkpoint,\n",
    "                                             lr_plateau\n",
    "                                             ],\n",
    "                                 verbose = 1)\n",
    "\n",
    "\n",
    "### progress ofr the first two epochs:\n",
    "### Epoch 1/21\n",
    "### 849/849 [==============================] - ETA: 0s - loss: 2.8980 - accuracy: 0.1618\n",
    "### Epoch 1: val_accuracy improved from -inf to 0.28744, saving model to ./tmp_checkpoint\\img_base_model_vgg\n",
    "### 849/849 [==============================] - 5998s 7s/step - loss: 2.8980 - accuracy: 0.1618 - val_loss: 2.4921 - val_accuracy: 0.2874 - lr: 0.0010\n",
    "### Epoch 2/21\n",
    "### 849/849 [==============================] - ETA: 0s - loss: 2.5013 - accuracy: 0.2619\n",
    "### Epoch 2: val_accuracy improved from 0.28744 to 0.30557, saving model to ./tmp_checkpoint\\img_base_model_vgg\n",
    "### 849/849 [==============================] - 5741s 7s/step - loss: 2.5013 - accuracy: 0.2619 - val_loss: 2.3451 - val_accuracy: 0.3056 - lr: 0.0010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cd64d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
